{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import HashingTF,IDF,Tokenizer,StopWordsRemover\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Apache-spark\\spark-3.2.3-bin-hadoop2.7\\python\\pyspark\\sql\\context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n"
     ]
    }
   ],
   "source": [
    "import pyspark as ps\n",
    "from pyspark.sql import SQLContext\n",
    "#import warnings\n",
    "#try:\n",
    "    # create SparkContext on all CPUs available: in my case I have 4 CPUs on my laptop\n",
    " #   sc = ps.SparkContext('local[*]')\n",
    "  #  sqlContext = SQLContext(sc)\n",
    "   # print(\"Just created a SparkContext\")\n",
    "##except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkversion 3.2.3\n"
     ]
    }
   ],
   "source": [
    "#create or get Spark Session\n",
    "#spark = sparknlp.start()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"spar\")\\\n",
    "    .master(\"spark://spark:7077\")\\\n",
    "    .config(\"spark.driver.memory\",\"8G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"500m\")\\\n",
    "    .getOrCreate()\n",
    "print(\"sparkversion\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il transforme les phrases du document en mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words Remover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words comme I, we, me, et ainsi de suite ont été supprimés car ils n’affectent pas le sens du mot.\n",
    "Il supprime la ponctuation et transforme les mots en minuscules. Ensuite, transformez ces mots propres documents en mots réguliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "swr = StopWordsRemover(inputCol=\"token\",outputCol=\"token_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il transforme les mots en valeurs numériques, ce processus est appelé vectorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"token_features\", outputCol=\"rawFeatures\")# To generate Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C’est un algorithme d’apprentissage automatique bien connu utilisé dans la classification. Il est utilisé pour mapper les caractéristiques numériques au sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC = LinearSVC(labelCol = 'label',featuresCol=\"features\",maxIter=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to make a full pipeline…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline().setStages([tokenizer,swr,hashingTF,idf,SVC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = spark.read.csv('train.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "| id|label|               tweet|\n",
      "+---+-----+--------------------+\n",
      "|  1|    0| @user when a fat...|\n",
      "|  2|    0|@user @user thank...|\n",
      "|  3|    0|  bihday your maj...|\n",
      "|  4|    0|#model   i love u...|\n",
      "|  5|    0| factsguide: soci...|\n",
      "|  6|    0|[2/2] huge fan fa...|\n",
      "|  7|    0| @user camping to...|\n",
      "|  8|    0|the next school y...|\n",
      "|  9|    0|we won!!! love th...|\n",
      "| 10|    0| @user @user welc...|\n",
      "| 11|    0| â #ireland con...|\n",
      "| 12|    0|we are so selfish...|\n",
      "| 13|    0|i get to see my d...|\n",
      "| 14|    1|@user #cnn calls ...|\n",
      "| 15|    1|no comment!  in #...|\n",
      "| 16|    0|ouch...junior is ...|\n",
      "| 17|    0|i am thankful for...|\n",
      "| 18|    1|retweet if you ag...|\n",
      "| 19|    0|its #friday! ð...|\n",
      "| 20|    0|as we all know, e...|\n",
      "+---+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet',r'http\\S+',''))\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','@\\w+',''))\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','#',''))\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','RT',''))\n",
    "\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','&lt',''))\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','&gt',''))\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','&amp',''))\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','&quot',''))\n",
    "\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','-',''))\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet',' ',''))\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','  ',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "| id|label|               tweet|\n",
      "+---+-----+--------------------+\n",
      "|  1|    0|whenafatherisdysf...|\n",
      "|  2|    0|thanksforlyftcred...|\n",
      "|  3|    0|   bihdayyourmajesty|\n",
      "|  4|    0|modeliloveutakewi...|\n",
      "+---+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_785686941cbd"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1072\\2060922449.py:2: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.info(pretty=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'ab62d2783536', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'XyzgtYCEQ9m5rf9Vrgywmg', 'version': {'number': '7.17.7', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '78dcaaa8cee33438b91eca7f5c7f56a70fec9e80', 'build_date': '2022-10-17T15:29:54.167373105Z', 'build_snapshot': False, 'lucene_version': '8.11.1', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "es.info(pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:kafka.coordinator.consumer:group_id is None: disabling auto-commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|\" : They\\u2019ve ...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1072\\1712011376.py:41: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|\" : Honble sir pl...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1072\\1712011376.py:41: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|\" : COVID is not ...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1072\\1712011376.py:41: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|\" : Crematoriums ...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1072\\1712011376.py:41: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|\" Agree with taxi...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1072\\1712011376.py:41: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|\" They are in the...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1072\\1712011376.py:41: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|\" : If you were O...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1072\\1712011376.py:41: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|\"WASHINGTON (AP) ...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1072\\1712011376.py:41: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|\" : Published in ...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1072\\1712011376.py:41: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|\" : Please retwee...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1072\\1712011376.py:41: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|\" : Funny how, wh...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1072\\1712011376.py:41: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|\"China didn\\u2019...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1072\\1712011376.py:41: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|\" : Just saw an a...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1072\\1712011376.py:41: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tweet|prediction|\n",
      "+--------------------+----------+\n",
      "|\" : Breaking: Isr...|       0.0|\n",
      "+--------------------+----------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_1072\\1712011376.py:41: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.index(\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "#import pydoop.hdfs as hdfs\n",
    "consumer = KafkaConsumer('covid',bootstrap_servers=['localhost:9092'])\n",
    "\n",
    "import pandas as pd\n",
    "columns = ['id','tweet', 'label']\n",
    "\n",
    "for message in consumer:\n",
    "   #print(i) \n",
    "   values = message.value.decode('utf-8').splitlines()\n",
    "   dfObj = pd.DataFrame(values,columns = ['tweet']) \n",
    "   dfObj.reset_index(drop=True) \n",
    "   dfObj=spark.createDataFrame(dfObj) \n",
    "    \n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet',r'http\\S+',''))\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','@\\w+',''))\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','#',''))\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','RT',''))\n",
    "\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','&lt',''))\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','&gt',''))\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','&amp',''))\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','&quot',''))\n",
    "\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','-',' '))\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','  ',' '))\n",
    "   #print(dfObj.select('tweet').show())\n",
    "   result=processed_tweets.transform(dfObj)\n",
    "   t=result.select('tweet').collect()\n",
    "   p=result.select('prediction').collect()\n",
    "   print(result.select(['tweet','prediction']).show())\n",
    "   ##################################################\n",
    "    ##################################################\n",
    "   #results = processed_tweets.transform(test_data)\n",
    "   from datetime import datetime\n",
    "   # Getting the current date and time\n",
    "   dt = datetime.now()\n",
    "   # getting the timestamp\n",
    "   ts = datetime.timestamp(dt) \n",
    "    # write data\n",
    "   es.index(\n",
    "         index='tweets_mondial',\n",
    "         document={\n",
    "          'tweet': t[0]['tweet'],\n",
    "          'prediction': p[0]['prediction'],\n",
    "          'Timestamp' :ts\n",
    " })\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "________________________________________________________________________________________________\n",
      "\n",
      "  Spark Streaming's Kafka libraries not found in class path. Try one of the following.\n",
      "\n",
      "  1. Include the Kafka library and its dependencies with in the\n",
      "     spark-submit command as\n",
      "\n",
      "     $ bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8:3.2.3 ...\n",
      "\n",
      "  2. Download the JAR of the artifact from Maven Central http://search.maven.org/,\n",
      "     Group Id = org.apache.spark, Artifact Id = spark-streaming-kafka-0-8-assembly, Version = 3.2.3.\n",
      "     Then, include the jar in the spark-submit command as\n",
      "\n",
      "     $ bin/spark-submit --jars <spark-streaming-kafka-0-8-assembly.jar> ...\n",
      "\n",
      "________________________________________________________________________________________________\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate();\n\u001b[0;32m      4\u001b[0m ssc \u001b[38;5;241m=\u001b[39m StreamingContext(sc, \u001b[38;5;241m10\u001b[39m) \u001b[38;5;66;03m# 10 second window\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m kvs \u001b[38;5;241m=\u001b[39m \u001b[43mKafkaUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDirectStream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mssc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtopics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcovid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkafkaParams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata.broker.list\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocalhost:9092\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Apache-spark\\spark-3.2.3-bin-hadoop2.7\\python\\pyspark\\streaming\\kafka.py:122\u001b[0m, in \u001b[0;36mKafkaUtils.createDirectStream\u001b[1;34m(ssc, topics, kafkaParams, fromOffsets, keyDecoder, valueDecoder, messageHandler)\u001b[0m\n\u001b[0;32m    119\u001b[0m     m\u001b[38;5;241m.\u001b[39m_set_value_decoder(valueDecoder)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m messageHandler(m)\n\u001b[1;32m--> 122\u001b[0m helper \u001b[38;5;241m=\u001b[39m \u001b[43mKafkaUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mssc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m jfromOffsets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(k\u001b[38;5;241m.\u001b[39m_jTopicAndPartition(helper),\n\u001b[0;32m    125\u001b[0m                       v) \u001b[38;5;28;01mfor\u001b[39;00m (k, v) \u001b[38;5;129;01min\u001b[39;00m fromOffsets\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messageHandler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Apache-spark\\spark-3.2.3-bin-hadoop2.7\\python\\pyspark\\streaming\\kafka.py:195\u001b[0m, in \u001b[0;36mKafkaUtils._get_helper\u001b[1;34m(sc)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_helper\u001b[39m(sc):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreaming\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkafka\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKafkaUtilsPythonHelper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJavaPackage\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object is not callable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "#sc = SparkContext(\"spark://spark:7077\", \"App Name\")\n",
    "sc = SparkContext.getOrCreate();\n",
    "ssc = StreamingContext(sc, 10) # 10 second window\n",
    "kvs = KafkaUtils.createDirectStream(ssc,topics=['covid'],kafkaParams= {\"metadata.broker.list\":'localhost:9092'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
