{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Using cached findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import HashingTF,IDF\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark-nlp\n",
      "  Downloading spark_nlp-4.2.5-py2.py3-none-any.whl (453 kB)\n",
      "Installing collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-4.2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spark-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Apache-spark\\spark-3.2.3-bin-hadoop2.7\\python\\pyspark\\sql\\context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n"
     ]
    }
   ],
   "source": [
    "import pyspark as ps\n",
    "from pyspark.sql import SQLContext\n",
    "import warnings\n",
    "try:\n",
    "    # create SparkContext on all CPUs available: in my case I have 4 CPUs on my laptop\n",
    "    sc = ps.SparkContext('local[*]')\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparknlp version 4.2.5 sparkversion 3.2.3\n"
     ]
    }
   ],
   "source": [
    "#create or get Spark Session\n",
    "#spark = sparknlp.start()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"sparkNLP\")\\\n",
    "    .master(\"spark://spark:7077\")\\\n",
    "    .config(\"spark.driver.memory\",\"8G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.5\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"500m\")\\\n",
    "    .getOrCreate()\n",
    "print(\"sparknlp version\", sparknlp.version(), \"sparkversion\", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Assembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> It turned tweets into documents. This step is considered as an entry to use sparkNLP library functions and annotators</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t2. Attaching DocumentAssembler Transformer to the pipeline\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "documentAssembler = sparknlp.base.DocumentAssembler().setInputCol('tweet').setOutputCol('document')\n",
    "print(f\"\\n\\t2. Attaching DocumentAssembler Transformer to the pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used this function to detect the multiple sentences from the tweet. However, there are more accurate and deep sentence detection and spell checkers could be used but need a lot of time and computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dentence_detector=SentenceDetector().setInputCols(['document']).setOutputCol('sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns the document sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer().setInputCols([\"sentence\"]).setOutputCol (\"token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words Remover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop words like I, we, me, and so on were removed as they don’t affect the meaning of the word.\n",
    "It removes punctuation and turns words into lower case. Then, turn these clean words documents into regular words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer().setInputCols([\"token\"]).setOutputCol ('normalized').setLowercase (True)\n",
    "\n",
    "finisher = Finisher().setInputCols([\"normalized\"]).setOutputCols([\"token_features\"]).setOutputAsArray(True) \\\n",
    ".setCleanAnnotations(False)# To generate Term Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns the words into numerical values, this process is called vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"token_features\", outputCol=\"rawFeatures\")# To generate Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s a well-known machine-learning algorithm used in classification. It’s used to map the numeric features to the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC = LinearSVC(labelCol = 'label',featuresCol=\"features\",maxIter=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to make a full pipeline…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline().setStages([documentAssembler,dentence_detector,tokenizer,normalizer,finisher,hashingTF,idf,SVC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = spark.read.csv('train.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "| id|label|               tweet|\n",
      "+---+-----+--------------------+\n",
      "|  1|    0| @user when a fat...|\n",
      "|  2|    0|@user @user thank...|\n",
      "|  3|    0|  bihday your maj...|\n",
      "|  4|    0|#model   i love u...|\n",
      "|  5|    0| factsguide: soci...|\n",
      "|  6|    0|[2/2] huge fan fa...|\n",
      "|  7|    0| @user camping to...|\n",
      "|  8|    0|the next school y...|\n",
      "|  9|    0|we won!!! love th...|\n",
      "| 10|    0| @user @user welc...|\n",
      "| 11|    0| â #ireland con...|\n",
      "| 12|    0|we are so selfish...|\n",
      "| 13|    0|i get to see my d...|\n",
      "| 14|    1|@user #cnn calls ...|\n",
      "| 15|    1|no comment!  in #...|\n",
      "| 16|    0|ouch...junior is ...|\n",
      "| 17|    0|i am thankful for...|\n",
      "| 18|    1|retweet if you ag...|\n",
      "| 19|    0|its #friday! ð...|\n",
      "| 20|    0|as we all know, e...|\n",
      "+---+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet',r'http\\S+',''))\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','@\\w+',''))\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','#',''))\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','RT',''))\n",
    "\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','&lt',''))\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','&gt',''))\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','&amp',''))\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','&quot',''))\n",
    "\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','-',''))\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet',' ',''))\n",
    "training_data=training_data.withColumn('tweet',F.regexp_replace('tweet','  ',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "| id|label|               tweet|\n",
      "+---+-----+--------------------+\n",
      "|  1|    0|whenafatherisdysf...|\n",
      "|  2|    0|thanksforlyftcred...|\n",
      "|  3|    0|   bihdayyourmajesty|\n",
      "|  4|    0|modeliloveutakewi...|\n",
      "+---+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tweets = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_d92e9306a465"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_3784\\2060922449.py:2: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es.info(pretty=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'ab62d2783536', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'XyzgtYCEQ9m5rf9Vrgywmg', 'version': {'number': '7.17.7', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '78dcaaa8cee33438b91eca7f5c7f56a70fec9e80', 'build_date': '2022-10-17T15:29:54.167373105Z', 'build_snapshot': False, 'lucene_version': '8.11.1', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "es.info(pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               tweet|\n",
      "+--------------------+\n",
      "|\" : \\u2018Anthony...|\n",
      "+--------------------+\n",
      "\n",
      "None\n",
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       1.0|\n",
      "+----------+\n",
      "\n",
      "None\n",
      "+--------------------+\n",
      "|               tweet|\n",
      "+--------------------+\n",
      "|\" : Requesting Al...|\n",
      "+--------------------+\n",
      "\n",
      "None\n",
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "+----------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Apache-spark\\spark-3.2.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"C:\\Apache-spark\\spark-3.2.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"C:\\Users\\pc\\anaconda3\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet',' ',''))\u001b[39;00m\n\u001b[0;32m     29\u001b[0m dfObj\u001b[38;5;241m=\u001b[39mdfObj\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m,F\u001b[38;5;241m.\u001b[39mregexp_replace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdfObj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtweet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     31\u001b[0m result\u001b[38;5;241m=\u001b[39mprocessed_tweets\u001b[38;5;241m.\u001b[39mtransform(dfObj)\n\u001b[0;32m     32\u001b[0m t\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtweet\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32mC:\\Apache-spark\\spark-3.2.3-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py:494\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Apache-spark\\spark-3.2.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mC:\\Apache-spark\\spark-3.2.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32mC:\\Apache-spark\\spark-3.2.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "#import pydoop.hdfs as hdfs\n",
    "consumer = KafkaConsumer('covid',bootstrap_servers=['localhost:9092'])\n",
    "\n",
    "import pandas as pd\n",
    "columns = ['id','tweet', 'label']\n",
    "\n",
    "for message in consumer:\n",
    "   #print(i) \n",
    "   values = message.value.decode('utf-8').splitlines()\n",
    "   dfObj = pd.DataFrame(values,columns = ['tweet']) \n",
    "   dfObj.reset_index(drop=True) \n",
    "   dfObj=spark.createDataFrame(dfObj) \n",
    "    \n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet',r'http\\S+',''))\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','@\\w+',''))\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','#',''))\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','RT',''))\n",
    "\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','&lt',''))\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','&gt',''))\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','&amp',''))\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','&quot',''))\n",
    "\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','-',' '))\n",
    "   dfObj=dfObj.withColumn('tweet',F.regexp_replace('tweet','  ',' '))\n",
    "   #print(dfObj.select('tweet').show())\n",
    "   result=processed_tweets.transform(dfObj)\n",
    "   t=result.select('tweet').collect()\n",
    "   p=result.select('prediction').collect()\n",
    "   print(result.select(['tweet','prediction']).show())\n",
    "   ##################################################\n",
    "    ##################################################\n",
    "   #results = processed_tweets.transform(test_data)\n",
    "   from datetime import datetime\n",
    "   # Getting the current date and time\n",
    "   dt = datetime.now()\n",
    "   # getting the timestamp\n",
    "   ts = datetime.timestamp(dt) \n",
    "    # write data\n",
    "   es.index(\n",
    "         index='tweets_mondial',\n",
    "         document={\n",
    "          'tweet': t[0]['tweet'],\n",
    "          'prediction': p[0]['prediction'],\n",
    "          'Timestamp' :ts\n",
    " })\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_analys = {\n",
    "    # this mapping definition sets up the fields for the rating events\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"tweet\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"prediction\": {\n",
    "                \"type\": \"integer\"\n",
    "            }\n",
    "        }  \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_10896\\3733732118.py:2: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  res_tweet = es.indices.create(index=\"tweet\", body=create_analys)\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_10896\\3733732118.py:2: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  res_tweet = es.indices.create(index=\"tweet\", body=create_analys)\n"
     ]
    }
   ],
   "source": [
    "# create indices with the settings and mappings above\n",
    "res_tweet = es.indices.create(index=\"tweet\", body=create_analys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
